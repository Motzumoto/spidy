THREAD_COUNT = 50
OVERWRITE = False
RAISE_ERRORS = False
SAVE_PAGES = True
SAVE_WORDS = True
ZIP_FILES = True
OVERRIDE_SIZE = True
RESTRICT = False
DOMAIN = ''
RESPECT_ROBOTS = False
TODO_FILE = 'crawler_todo.txt'
DONE_FILE = 'crawler_done.txt'
WORD_FILE = 'crawler_words.txt'
SAVE_COUNT = 100
HEADER = HEADERS['spidy']
MAX_NEW_ERRORS = 30
MAX_KNOWN_ERRORS = 30
MAX_HTTP_ERRORS = 30
MAX_NEW_MIMES = 30
START = ["https://www.google.com/", "https://www.youtube.com/"]
